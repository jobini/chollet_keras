{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.compat.v2.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights=\"imagenet\")\n",
    "img_path = \"/home/rtx/jobin/chollet_keras/data/elephants.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(img_path,target_size=(224,224))\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x,axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n02504458', 'African_elephant', 0.87656206),\n",
       "  ('n01871265', 'tusker', 0.11420009),\n",
       "  ('n02504013', 'Indian_elephant', 0.009071711)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_predictions(preds,top=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_model = Model(model.input,[model.output,model.get_layer(\"block5_conv3\").output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, last_conv_layer_np = activation_model.predict(x)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(tf.convert_to_tensor(x))\n",
    "    preds,layer_out = activation_model(x)\n",
    "    african_elephant_output = preds[0][386]\n",
    "\n",
    "grads = tape.gradient(african_elephant_output,layer_out)\n",
    "pooled_grads = tf.reduce_mean(grads,axis=(0,1,2))\n",
    "\n",
    "for i in range(512):\n",
    "    last_conv_layer_np[0][:,:,i] *= pooled_grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14, 14, 512)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_conv_layer_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f38344cb3c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap = np.mean(last_conv_layer_np,axis=-1)\n",
    "heatmap = np.maximum(heatmap,0)\n",
    "heatmap /= np.max(heatmap)\n",
    "heatmap = heatmap[0]\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(img_path)\n",
    "heatmap = cv2.resize(heatmap,(img.shape[1],img.shape[0]))\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap,cv2.COLORMAP_JET)\n",
    "superimposed_image = heatmap * 0.6 + img\n",
    "cv2.imwrite(\"elephant_cam.jpg\",superimposed_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
